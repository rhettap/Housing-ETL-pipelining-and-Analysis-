{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3c20f5",
   "metadata": {
    "papermill": {
     "duration": 0.005972,
     "end_time": "2023-04-13T12:14:07.323096",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.317124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Housing Data in Greenville \n",
    "\n",
    "So for this project, I wanted to get more practice with the entire pipeline process from analytics engineering to data analysis. I'm more of an analyst type of guy, but I wanted to try my hand at doing some engineering work.\n",
    "\n",
    "I recently visited Greenville, South Carolina and really enjoyed the city and it's proximity to the mountains. Just for fun, I wanted to learn more about rental costs and what house prices looked like if I wanted to relocate to the area. I'm also curious about how prices vary in different zipcodes around Greenville.\n",
    "\n",
    "I'm going to follow a pipelining ELT process by first extracting data from my source(s),loading the data into a warehouse, and then transforming the data using dbt. Afterwards, I'll run some analysis on the data and do some visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c1b5d",
   "metadata": {
    "papermill": {
     "duration": 0.003692,
     "end_time": "2023-04-13T12:14:07.331143",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.327451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ELT process stage - Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3329b",
   "metadata": {
    "papermill": {
     "duration": 0.003479,
     "end_time": "2023-04-13T12:14:07.338574",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.335095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Investigating and Compiling from the Data Source\n",
    "\n",
    "I navigated to RedFin's website to find information about houses for sale, ones that have sold, and rentals available. The link is here: https://www.redfin.com/city/7891/SC/Greenville. \n",
    "\n",
    "**Note**: some of the numbers / addresses may be outdated from what you see in my project as I've collected the data on March 12, 2023. \n",
    "\n",
    "The first thing I noticed is that they had a free CSV file download for their Houses for Sale and Sold Houses data. Perfect! \n",
    "\n",
    "[![For-sale-homes.png](https://i.postimg.cc/sfWRDGWM/For-sale-homes.png)](https://postimg.cc/XXnPsJQW)\n",
    "\n",
    "[![Sold-homes.png](https://i.postimg.cc/nV2Gm0Bg/Sold-homes.png)](https://postimg.cc/kDVS3Njv)\n",
    "\n",
    "This saved me a lot of time with data collection for these categories. \n",
    "\n",
    "When I went to look at the rentals data, however, this wasn't the case. \n",
    "\n",
    "[![Rentals.png](https://i.postimg.cc/nV9QF2Tq/Rentals.png)](https://postimg.cc/qg07Q8P7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71099a51",
   "metadata": {
    "papermill": {
     "duration": 0.003483,
     "end_time": "2023-04-13T12:14:07.345928",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.342445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Web Scraping\n",
    "________________________________________________________________________________________________\n",
    "**Disclaimer**: \n",
    "\n",
    "There is some debate regarding the legality of webscraping. In short, any public facing data can legally be scraped. I always strive to do things in a proper/legal manner, so I just wanted to add this short disclaimer to clear up any confusion. Thanks for understanding! \n",
    "\n",
    "I'm not selling this data or using it in any sort of business venture. This is strictly being used for my own personal project to practice data pipelining. I don't advise or authorize anyone to use the data I've collected in a way that could turn a profit or further a business venture. I haven't logged in to the website, or agreed to any terms of service. I've simply collected public facing data automatically as opposed to writing down the information manually into an excel spreadsheet. I've also only scraped the data once and haven't set up a bot to repeatedly pull data.  I've read a lot online about the legality of web scraping and feel I have done this in the appropriate manner. Please let me know in the comments if you think there are any issues, and I will be more than happy to conform to any standard practices of legality.\n",
    "________________________________________________________________________________________________\n",
    "\n",
    "I decided to inspect the data and look at the underlying HTML/CSS. \n",
    "\n",
    "There I found some elements pertaining to the data I was looking to extract. \n",
    "\n",
    "[![CSS-columns.png](https://i.postimg.cc/xC0tr7LG/CSS-columns.png)](https://postimg.cc/Z9Mrpw70)\n",
    "\n",
    "I went to Jupyter notebooks and began to code out the logic to extract this data using Selenium and Chromedriver. \n",
    "\n",
    "**Note**: I'm not going to post the code directly in this notebook, because I don't want the code to be forked and then repeatedly triggered on RedFin. I've opted for screenshots instead for this particular project. Thanks for understanding!\n",
    "\n",
    "\n",
    "[![jupyter-scraping-1.png](https://i.postimg.cc/CKK9kzQp/jupyter-scraping-1.png)](https://postimg.cc/dhzWKtF5)\n",
    "\n",
    "First I imported all of the associated libraries that I thought I might need and set up the paths. \n",
    "\n",
    "I switched from photo to table format, so that I could extract the data I wanted. \n",
    "\n",
    "[![jupyter-scraping-2.png](https://i.postimg.cc/sXTXKQVP/jupyter-scraping-2.png)](https://postimg.cc/mzFBgr1h)\n",
    "\n",
    "Next, I inspected the page and found the table element. I assigned this to the variable \"rentals\" and then made empty lists for each component I wanted to extract. \n",
    "\n",
    "I looped through each \"rental\" in the \"rentals\" table and extracted the information by locating each element by class. This appended all of the appropriate information to the corresponding lists for the first page. \n",
    "\n",
    "[![jupyter-scraping-3.png](https://i.postimg.cc/Kjb6KJXJ/jupyter-scraping-3.png)](https://postimg.cc/xJsxhG1z)\n",
    "\n",
    "While I now have all of the data from the first page, there are several other pages of data that I have not yet compiled. To do this, I found the next button and created a while loop to continually click the next button and scrape the data from the pages. Whenever the loop finished, it would close the page and break the loop. \n",
    "\n",
    "[![jupyter-scraping-4.png](https://i.postimg.cc/TwdjpD7n/jupyter-scraping-4.png)](https://postimg.cc/HJN8NVCL)\n",
    "\n",
    "Finally, after I had compiled all of the data, I printed the number of items (rows) for each list to match the number of 177 rentals that I found online. It looks like we were successful! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdfbfe",
   "metadata": {
    "papermill": {
     "duration": 0.003417,
     "end_time": "2023-04-13T12:14:07.353124",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.349707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating a DataFrame and CSV file \n",
    "\n",
    "This section will be short, we're basicallly going to compile all of the data gathered about rental houses into a dataframe in Python and then save the CSV file with our other data. \n",
    "\n",
    "[![creating-a-dataframe.png](https://i.postimg.cc/dVmvrwbm/creating-a-dataframe.png)](https://postimg.cc/k24kd3XB)\n",
    "\n",
    "We have our dataframe and all of the rows and columns. Now let's save it as a CSV file on my local computer. \n",
    "\n",
    "[![CSV-file.png](https://i.postimg.cc/MHQmXWs1/CSV-file.png)](https://postimg.cc/ZvJdMhzq)\n",
    "\n",
    "[![all-csv-data-files.png](https://i.postimg.cc/TP3S1Dnm/all-csv-data-files.png)](https://postimg.cc/9Ds1Nz8F)\n",
    "\n",
    "And now we have all of our data extracted and ready to be cleaned up and organized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383f52a",
   "metadata": {
    "papermill": {
     "duration": 0.003422,
     "end_time": "2023-04-13T12:14:07.360263",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.356841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cleaning in Google Sheets / Excel\n",
    "\n",
    "Now that I have all of my raw data, I want to do some minor transformations (specifically to the rental data). I noticed that the rental data doesn't have a zip code column, and I would like the zipcodes for further down the line when I will be doing geographic visualizations. After a bit of research, I think I've found a way to automatically fill in the zip using the full address. To do this I'm going to first use Google Sheets and then move the data back to Excel, since I'm more familiar with Excel and don't know if this method will work in the same way. \n",
    "\n",
    "I've loaded the rental csv data into Google Sheets and am going to concatenate the address fields into one column called Full Address. \n",
    "\n",
    "[![Concatenate-Address.png](https://i.postimg.cc/dVnY1vZ7/Concatenate-Address.png)](https://postimg.cc/4YKMLD5Z)\n",
    "\n",
    "Next, I found a function online that can use geographic information (address,city,state) to locate a zip code. \n",
    "\n",
    "[![Geocode-function.png](https://i.postimg.cc/jjnCJZ0Y/Geocode-function.png)](https://postimg.cc/B8sJr5nm)\n",
    "\n",
    "Now I'm just going to plug this into my newly created ZIP CODE column and see if it works.\n",
    "\n",
    "[![function-in-aciton.png](https://i.postimg.cc/0Qq5Kv07/function-in-aciton.png)](https://postimg.cc/WDXcxQK4)\n",
    "\n",
    "[![full-list.png](https://i.postimg.cc/L4D45rYZ/full-list.png)](https://postimg.cc/grwbBtKY)\n",
    "\n",
    "And it appears to have worked beautifully! Let me go back to the house data and cross reference some of the addresses just to double check. \n",
    "\n",
    "[![zipcode-accuracy-1.png](https://i.postimg.cc/HL7TxTn9/zipcode-accuracy-1.png)](https://postimg.cc/CZY3rTjR)\n",
    "\n",
    "[![zipcode-accuracy-2.png](https://i.postimg.cc/3JShFkDZ/zipcode-accuracy-2.png)](https://postimg.cc/HVyFpszJ)\n",
    "\n",
    "After cross referencing several address zipcodes with the same address on the site, I feel comfortable saying that the zipcodes are accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e2a47",
   "metadata": {
    "papermill": {
     "duration": 0.003369,
     "end_time": "2023-04-13T12:14:07.367402",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.364033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now I'm going to pull each CSV file into Excel. They main reason I want to bring them into Excel is to make sure they all have matching column names and to separate out the addresses into different sections, so they can be aggregated later when visualizing. \n",
    "\n",
    "I won't show the same repeated steps for each CSV file. \n",
    "\n",
    "[![Parsing-out-the-street-name.png](https://i.postimg.cc/659CkJRj/Parsing-out-the-street-name.png)](https://postimg.cc/SjvX9PgC)\n",
    "\n",
    "Parsing the street name from the address field.\n",
    "\n",
    "[![Post-Date.png](https://i.postimg.cc/prPkc9Rk/Post-Date.png)](https://postimg.cc/ygrFk8JS)\n",
    "\n",
    "Creating a calculated column for when the listing was posted. \n",
    "\n",
    "[![category.png](https://i.postimg.cc/FHxSGP7t/category.png)](https://postimg.cc/yDkW8ymn)\n",
    "\n",
    "Creating a category column to distinguish between the house posting types when I combine them into one big table later. \n",
    "\n",
    "[![Only-350-listings.png](https://i.postimg.cc/wjnCfkPV/Only-350-listings.png)](https://postimg.cc/Xrc1X9nG)\n",
    "\n",
    "On a side note, I've noticed that there are only 350 rows in the \"For Sale\" and \"SOLD\" csv files. There should have been 604 rows and 820 rows respectively.\n",
    "\n",
    "[![Data-limit-350-rows.png](https://i.postimg.cc/Xq8ZxqYf/Data-limit-350-rows.png)](https://postimg.cc/PvCr5tZJ)\n",
    "\n",
    " After doing a little digging, I found that RedFin allows a max download of 350 rows for these datasets. I could go back and scrape it, but I think 350 rows will be enough for what we want to do. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339fc7a",
   "metadata": {
    "papermill": {
     "duration": 0.00346,
     "end_time": "2023-04-13T12:14:07.374558",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.371098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ELT process stage - Loading the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe4b69",
   "metadata": {
    "papermill": {
     "duration": 0.003546,
     "end_time": "2023-04-13T12:14:07.381873",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.378327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Big Query \n",
    "\n",
    "Now that we have all of our raw data extracted and compiled into CSV files, I'm going to load it into Big Query Data Warehouse. \n",
    "\n",
    "[![Loading-into-Big-Query.png](https://i.postimg.cc/ZKz4M0Fh/Loading-into-Big-Query.png)](https://postimg.cc/0rnT6kBt)\n",
    "\n",
    "So I've created a new project (greenvilleproject), a dataset (housing_data) within the project, and uploaded all of the raw CSV files/tables into the the dataset. I ran a quick SQL query, double-checked the output, and it looks like we are all good to go. \n",
    "\n",
    "Next I need to generate a key in Big Query, so that I can connect to dbt cloud and transform my data.\n",
    "\n",
    "\n",
    "[![key.png](https://i.postimg.cc/BbRTN1w8/key.png)](https://postimg.cc/30C0xWh7)\n",
    "\n",
    "New Key has been created, time to connect it now with dbt using the json file. \n",
    "\n",
    "## dbt \n",
    "\n",
    "[![dbt-project-ready.png](https://i.postimg.cc/4yzBqPkC/dbt-project-ready.png)](https://postimg.cc/D4wrbqw5)\n",
    "\n",
    "Alrighty, I've connected the big query to dbt, set up an in house repository (which can be exported later to GitHub), and now my project is ready to begin! \n",
    "\n",
    "[![Models-folders-dbt.png](https://i.postimg.cc/6QCV905Y/Models-folders-dbt.png)](https://postimg.cc/062Kn7Sw)\n",
    "\n",
    "After creating a new branch in the repository, I've begun setting up folders for my different data stages and yml file to hold my sources. \n",
    "\n",
    "[![source.png](https://i.postimg.cc/TPQsdfPk/source.png)](https://postimg.cc/t1ZkD0WP)\n",
    "\n",
    "Next, I updated the .yml file to contain all of my source data and subsequently started building out my lineage from the source. \n",
    "\n",
    "[![staging-files-dbt.png](https://i.postimg.cc/vHz2LGrG/staging-files-dbt.png)](https://postimg.cc/mtPSTfpn)\n",
    "\n",
    "I then loaded the raw data files into staging files and connected them to the sources.\n",
    "It looks like all of the rows were returned. Great!\n",
    "\n",
    "After connecting all of the staging files with source data, I made a commit in my repo to document these changes. \n",
    "\n",
    "[![all-staged.png](https://i.postimg.cc/05m7qZj3/all-staged.png)](https://postimg.cc/z3zyCCnS)\n",
    "\n",
    "Finally, I'm going to change the materialization of my data in Big Query from views to tables. \n",
    "[![materialized.png](https://i.postimg.cc/T1rnV8Pz/materialized.png)](https://postimg.cc/QKdBDy1m)\n",
    "\n",
    "Just as a quick check to make sure everything is working as expected, I'm going to do a quick \"dbt run\" and see how things are looking over in big query. \n",
    "\n",
    "[![dbt-run-test.png](https://i.postimg.cc/MGNBgZfk/dbt-run-test.png)](https://postimg.cc/TyjhD6cQ)\n",
    "\n",
    "Run was a success! \n",
    "\n",
    "[![Big-query-first-transfer.png](https://i.postimg.cc/jSCLK0bR/Big-query-first-transfer.png)](https://postimg.cc/mPfbNnkn)\n",
    "\n",
    "And it looks like we're all good to go! While we technically did some minor tranformations to create staging tables, the \"real\" transformation process is going to happen next. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e9de5",
   "metadata": {
    "papermill": {
     "duration": 0.003435,
     "end_time": "2023-04-13T12:14:07.389149",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.385714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ELT Process Stage - Transforming the Data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc159808",
   "metadata": {
    "papermill": {
     "duration": 0.003413,
     "end_time": "2023-04-13T12:14:07.396277",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.392864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Quick aside** - I've just realized that I didn't add any sort of unique identifying columns to my data, so I went back and created primary key columns (with incrementing integer values) for each table in Excel and then re-uploaded the raw data and made sure the connection was still working fine. (ex: FS1 - For Sale; S1 - Sold; R1 - Rental)\n",
    "\n",
    "[![auto-incrementing.png](https://i.postimg.cc/9frpYJrX/auto-incrementing.png)](https://postimg.cc/KKhBZ7PC)\n",
    "\n",
    "__________________________________________________________________________________________________________\n",
    "\n",
    "## ERD\n",
    "First, I'm going to map out the dimension and fact tables using an ERD in draw.io. This way I can have a visual model to use as a guide when creating the ERD.\n",
    "\n",
    "**Note**- Since this data is all from static tables, these will all have a one to one relationship (cardinality). If the data were dynamic in nature then we could align it to have one to many relationships. Normally, one to one would be combined into a giant table for querying, which is what we will do in the next section, but just as an example I'm going to build out an ERD with this data. This is also why each table has the same primary key.\n",
    "\n",
    "[![ERD.png](https://i.postimg.cc/kMjbFRCv/ERD.png)](https://postimg.cc/ThD1TwpK)\n",
    "\n",
    "\n",
    "Now that we have all of our raw data staged in tables in dbt, let's run some more fine-grained SQL queries to extract specific subsets of the data that we mapped out in the ERD. \n",
    "\n",
    "First, I decided to create some dimension tables with columns relating to address fields. This includes columns such as id_column(primary key),address, address_street, city, state, zip, location, longitude, and latitude. \n",
    "\n",
    "[![dim-sold-address.png](https://i.postimg.cc/HW6vCcXy/dim-sold-address.png)](https://postimg.cc/w3sQXBWq)\n",
    "\n",
    "I've created these dimension tables from the for_sale, sold, and rentals staging tables and referenced them directly. This preserves our lineage with a modular sort of design.  \n",
    "\n",
    "[![sold-dates-model.png](https://i.postimg.cc/FHPNv3gn/sold-dates-model.png)](https://postimg.cc/gx6fqw7v)\n",
    "\n",
    "I've gone through and created dim_date models and fact tables from each staging table as well. In the interest of not overcomplicating this project, all of the lineages tie directly back to their source tables (no joins here).\n",
    "\n",
    "[![rentals-fact-model.png](https://i.postimg.cc/YShkC0d2/rentals-fact-model.png)](https://postimg.cc/3y5zL8Mz)\n",
    "\n",
    "On the note of modularity, I've separated all of the models' folders by staging, dimension(containing date and address modules) and facts.\n",
    "\n",
    "Here's what our current lineage and models' folder structure looks like: \n",
    "\n",
    "[![Screenshot-2023-03-15-at-5-25-45-PM.png](https://i.postimg.cc/bNH5cPH6/Screenshot-2023-03-15-at-5-25-45-PM.png)](https://postimg.cc/jL2cPG1N)\n",
    "\n",
    "Now that we've transformed a lot of the data and have it staged nicely, I'm going to make another commit and then do a \"dbt run\" to push it all back to Big Query. \n",
    "\n",
    "[![Big-query-models.png](https://i.postimg.cc/htNT2zzN/Big-query-models.png)](https://postimg.cc/68L83Qhh)\n",
    "\n",
    "Back in Big Query, it looks like all of our models transferred successfully!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b252a9d",
   "metadata": {
    "papermill": {
     "duration": 0.003446,
     "end_time": "2023-04-13T12:14:07.403437",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.399991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Analysis \n",
    "\n",
    "Now that we've extracted our raw data from public sources, loaded it into Big Query, created a sample ERD, and transformed it using dbt, it's time to do what I do best: data analysis. At this stage, I would pull all of these dim and fact tables into a tool, like Power BI, and do my data modeling and subsequent querying there. However, given the nature of this static data, I'm just going to create OBT (with sold and for sale data) and use Tableau to visualize it (as I prefer the visualization tool's map features to PBI).\n",
    "\n",
    "First I'm going to combine the raw_sold and raw_for_sale tables since they have a lot of commonalities not shared in the scraped rental table. This will be done directly in Big Query using a simple Union operator \n",
    "\n",
    "[![Union.png](https://i.postimg.cc/wB4Zbc59/Union.png)](https://postimg.cc/1g6v8FYd)\n",
    "\n",
    "And now checking to make sure we've returned 700 rows (350 from raw_sold + 350 from raw_for_sale). \n",
    "\n",
    "[![count-from-union.png](https://i.postimg.cc/sg6TVfMc/count-from-union.png)](https://postimg.cc/dhC26Ymk)\n",
    "\n",
    "Looks like we were successful! \n",
    "\n",
    "While we have this data available to query with SQL, I'm going to run a few quick queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896578dc",
   "metadata": {
    "papermill": {
     "duration": 0.003472,
     "end_time": "2023-04-13T12:14:07.410774",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.407302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SQL \n",
    "\n",
    "First, lets look at rental data. I want to know the 10 \"most popular\" streets to rent houses (by number of rentals available) and the average prices for rentals on these streets compared to the average prices across all rentals(streets). (Note: I went back to Excel and changed the range prices to the lower value, just for sake of ease.) \n",
    "\n",
    "[![SQL-0.png](https://i.postimg.cc/Xq3J73CQ/SQL-0.png)](https://postimg.cc/ft2ZBG59)\n",
    "\n",
    "Looks like Stonefence Dr. has quite a lot of rentals available with a much higher price than the average. I wonder if there are so many properties available to rent because the price is too high and no one wants to pay that much? \n",
    "\n",
    "I'm curious about the average, minimum, and maximum prices in the for_sale and sold categories broken up by the 5 most popular zip codes (the zip codes with the most number of for_sale and sold houses in them)? \n",
    "\n",
    "[![SQL-1.png](https://i.postimg.cc/Qt0Jk5P1/SQL-1.png)](https://postimg.cc/G8TDbBbm)\n",
    "\n",
    "It appears as though there were houses sold for 17,500, 28,500, 30,000, 43,000, and 60,000 USD in some of these popular zipcodes. That seems suspicious to me. I need to investigate the data more to find out why these prices were so low. \n",
    "\n",
    "[![SQL-2.png](https://i.postimg.cc/j5GDySct/SQL-2.png)](https://postimg.cc/sQ4frybN)\n",
    "\n",
    "I had to join this table back to the raw_sold table to get a full address, since I only added ADDRESS_STREET into my unioned table. (fsas alias = For Sale and Sold)\n",
    "\n",
    "It makes a lot more sense now that these are mostly vacant parcels of land. I am curious about 24 Ross St. as it seems this family got a steal by purchasing a home in the top 5 zipcodes for only $60,000. Let's go look on RedFin. \n",
    "\n",
    "[![Ross-st.png](https://i.postimg.cc/SQPF4tN0/Ross-st.png)](https://postimg.cc/G9vSJKyz)\n",
    "\n",
    "\n",
    "\n",
    "Looking at this listing, I can see why it was so cheap. It's quite small and it is going to have to be completed re-built. The posting notes that it is \"gutted to the studs\". The estimated sale price is over doubled what was paid which is interesting....\n",
    "\n",
    "I have several other previous projects where I've done some advanced SQL queries. So while I'd like to do more here, I'm afraid my notebook is getting too long and I still want to do some visualizations in Tableau. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba81061",
   "metadata": {
    "papermill": {
     "duration": 0.003465,
     "end_time": "2023-04-13T12:14:07.418019",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.414554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tableau \n",
    "\n",
    "I did a big Tableau project before on Kaggle exploring a lot of different features. For this project, I'm going to do just a few simple visualizations seen how it isn't the main focus of this particular project. I'm also just going to cater these visualizations / dashboards to **homes for sale**. \n",
    "\n",
    "[![tableau-1.png](https://i.postimg.cc/7LskC2WC/tableau-1.png)](https://postimg.cc/GByNVHrr)\n",
    "\n",
    "So I've pulled in that unioned table (for sale and sold info) into Tableau and begun by visualizing a map showing average for sale house prices by zip code. The 29687 zip must be a lot of those vacant land parcels that we saw earlier, as their average price is around 65,000USD. \n",
    "\n",
    "**Quick note** - Since I'm just showing visualizations with regards to \"for sale\" properties in this project, I could have pulled the for_sale.csv and not the combined one. However, I want to do some comparisons outside the scope of this project between the for_sale and sold data, so I pulled both. \n",
    "\n",
    "[![Property-type.png](https://i.postimg.cc/MH0Xvfsj/Property-type.png)](https://postimg.cc/FkRh6zX9)\n",
    "\n",
    "Next we looked at the various property types that are currently available for sale. The overwhelming majority are single family residential homes, which make sense.\n",
    "\n",
    "[![Screenshot-2023-03-16-at-4-36-07-PM.png](https://i.postimg.cc/8crk6Jtb/Screenshot-2023-03-16-at-4-36-07-PM.png)](https://postimg.cc/gLmP9J9w)\n",
    "\n",
    "I wanted to see what for sale listings have been looking like on a weekly basis for the past 3 months. It looks like mid-February to early march we had a major uptick in the number of listings, but mid-March brought a major dip. \n",
    "\n",
    "[![sqft.png](https://i.postimg.cc/7hJkt5GQ/sqft.png)](https://postimg.cc/y3KtkYTF)\n",
    "\n",
    "It looks like these are the top 10 streets with houses for sale that have the biggest sq_ft on average.\n",
    "\n",
    "I think this is enough info for our for sale houses, so I'm going to build it all out into a dashboard now. \n",
    "\n",
    "[![Screenshot-2023-03-16-at-5-31-14-PM.png](https://i.postimg.cc/ZRQ6rQhz/Screenshot-2023-03-16-at-5-31-14-PM.png)](https://postimg.cc/VrjSwD6K)\n",
    "\n",
    "Here's a quick dashboard I made combining our visualizations. If you want to play around with the interactive elements, you can find it here: \n",
    "\n",
    "https://public.tableau.com/views/GreenvilleHousing/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd9cd6",
   "metadata": {
    "papermill": {
     "duration": 0.003464,
     "end_time": "2023-04-13T12:14:07.425210",
     "exception": false,
     "start_time": "2023-04-13T12:14:07.421746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "This ends my analytics engineering and data analysis housing project. I've enjoyed getting a feel for these engineering tools. I realize this would be more practical if it was connected to a live data source that constantly updated with dynamic data, but nevertheless I'm still proud of what I've accomplished here. If you have any questions, advice, or just want to chat, please feel free to leave a comment below. Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.461254,
   "end_time": "2023-04-13T12:14:07.655311",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-13T12:13:54.194057",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
